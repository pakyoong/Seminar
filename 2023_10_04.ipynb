{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be9d05c",
   "metadata": {},
   "source": [
    "# Deep Learning-based Real-time Heart Rate Measurement System Using Mobile Facial Videos\n",
    "\n",
    "### 딥러닝 기반의 모바일 얼굴 영상을 이용한 실시간 심박수 측정 시스템\n",
    "\n",
    "- paper : https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE10667432"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f88286",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This paper proposes a new system to measure heart rate non-contactly and in real time using a mobile app.\n",
    "\n",
    "-  이 논문은 모바일 앱을 사용하여 심박수를 비접촉 방식과 실시간으로 측정하는 새로운 시스템을 제안합니다. \n",
    "\n",
    "This system uses facial images taken in various situations to predict photopismography and measures heart rate using it.\n",
    "\n",
    "- 이 시스템은 다양한 상황에서 촬영된 얼굴 이미지를 사용하여 광피스모그래피를 예측하고, 이를 통해 심박수를 측정합니다. \n",
    "\n",
    "Experimental results show that this system can accurately measure heart rate without significant differences compared to existing methods.\n",
    "\n",
    "- 실험 결과, 이 시스템은 기존의 방법과 비교하여 유의한 차이 없이 정확한 심박수 측정이 가능함을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0e5f3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Heart rate is an important indicator that reflects a person's biological and mental state.\n",
    "\n",
    "- 심박은 사람의 생체, 정신적 상태를 반영하는 중요한 지표입니다.\n",
    "\n",
    "Methods commonly used to measure cardiac activity include electrocardiogram (ECG) or photoplethysmography (PPG).\n",
    "\n",
    "- 일반적으로 심장 활동을 측정하기 위해 사용되는 방법은 심전도(ECG)나 광전용적맥파(PPG)가 있습니다.\n",
    "\n",
    "However, existing methods have the disadvantage of limiting movement and requiring sensors to be attached to the body.\n",
    "\n",
    "- 하지만 기존 방법들은 움직임을 제한하고 센서를 신체에 부착해야 한다는 단점이 있습니다.\n",
    "\n",
    "To solve this problem, a remote photoplethysmography (rPPG) method that estimates cardiac activity without contact has recently been attempted.\n",
    "\n",
    "- 이러한 문제를 해결하기 위해 최근 비접촉으로 심장 활동을 추정하는 원거리 광전용적맥파(remote PPG; rPPG) 방식이 시도되고 있습니다.\n",
    "\n",
    "rPPG is a non-contact heart rate measurement technology that estimates heart activity through color changes in facial skin.\n",
    "\n",
    "- rPPG는 심박을 비접촉으로 측정하는 기술로, 얼굴 피부의 색상 변화를 통해 심장 활동을 추정합니다.\n",
    "\n",
    "Although it has the potential to be used in various fields, it is currently difficult to apply it in daily life.\n",
    "\n",
    "- 다양한 분야에서 활용 가능성이 있지만, 현재는 일상생활에서의 적용이 어렵습니다.\n",
    "\n",
    "Using deep learning, you can automatically extract heart rate-related features from face images.\n",
    "\n",
    "- 딥러닝을 활용하면 얼굴 영상에서 자동으로 심박 관련 특징을 추출할 수 있습니다.\n",
    "\n",
    "Deep learning is robust against noise and can be applied in real environments.\n",
    "\n",
    "- 딥러닝은 잡음에 대해 견고하며, 실제 환경에서의 적용이 가능합니다.\n",
    "\n",
    "In this paper, we propose a system to measure heart rate non-contactly using a smartphone camera.\n",
    "\n",
    "-  본 논문에서는 스마트폰 카메라를 이용하여 심박을 비접촉으로 측정하는 시스템을 제안합니다.\n",
    "\n",
    "Verify the operability of the proposed system in the real environment by conducting experiments under various real-life conditions.\n",
    "\n",
    "- 실생활의 다양한 조건에서 실험하여 제안하는 시스템의 실제 환경에서의 작동 가능성을 검증합니다.\n",
    "\n",
    "The proposed system performs data collection and processing simultaneously on smartphones, enabling real-time processing and protecting data.\n",
    "\n",
    "- 제안된 시스템은 스마트폰에서 데이터 수집과 처리를 동시에 수행하여 실시간 처리를 가능하게 하고 데이터를 보호합니다.\n",
    "\n",
    "The performance of the proposed system was evaluated in comparison with a pulse oximeter.\n",
    "\n",
    "- 제안된 시스템의 성능은 펄스 옥시미터와 비교하여 평가되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500085d0",
   "metadata": {},
   "source": [
    "## Related Research\n",
    "\n",
    "\n",
    "Traditional ECG is the most researched method, recording electrical activity of the heart, utilized primarily in medical institutions.\n",
    "\n",
    "- 전통적인 ECG는 가장 많이 연구된 방법으로, 주로 의료 기관에서 사용되며 심장의 전기적 활동을 기록합니다.\n",
    "\n",
    "Despite its high accuracy, it has limitations such as the need for special equipment and restrictions on mobility.\n",
    "\n",
    "- 높은 정확도에도 불구하고 별도의 장비 필요성과 이동성 제약과 같은 한계가 있습니다.\n",
    "\n",
    "Recent proposals involve identifying heartbeat signals from radar reflections modulated by heartbeats using high-sensitivity radar sensors.\n",
    "\n",
    "- 최근의 제안들은 심장 박동에 의해 변조된 레이더 반사 신호에서 심박 신호를 식별하는 고감도 레이더 센서를 사용합니다.\n",
    "\n",
    "However, these sensors only work in designated spaces, not solving the mobility issue.\n",
    "\n",
    "- 그러나 이러한 센서들은 지정된 공간에서만 작동하여 이동성 문제를 해결하지 못합니다.\n",
    "\n",
    "Another method, PPG, estimates heart rate signals by sensing changes in blood volume as a time-series signal.\n",
    "\n",
    "- 또 다른 방법인 PPG는 시계열 신호로서 혈류량의 변화를 감지하여 심박 신호를 추정합니다.\n",
    "\n",
    "Despite its portability and integration into wearable devices, it requires skin contact, making it unsuitable for non-contact biometric signal measurement systems.\n",
    "\n",
    "- 이동성과 웨어러블 기기로의 통합에도 불구하고 피부 접촉이 필요하여 비접촉 생체신호 측정 시스템에는 적합하지 않습니다.\n",
    "\n",
    "The employed method in the proposed system, rPPG, is based on the principle of PPG, capturing the change in skin color using an RGB camera for non-contact heart rate measurement.\n",
    "\n",
    "- 제안된 시스템에서 사용된 방법인 rPPG는 PPG의 원리를 기반으로 RGB 카메라를 사용하여 피부색의 변화를 포착함으로써 비접촉 심박 측정을 합니다.\n",
    "\n",
    "Several studies have measured rPPG signals from mobile facial images using enhanced smartphone front cameras, emphasizing the role of smartphones in remote heart rate measurement.\n",
    "\n",
    "- 여러 연구들은 향상된 스마트폰 전면 카메라를 사용하여 모바일 얼굴 이미지에서 rPPG 신호를 측정하였으며 스마트폰의 원격 심박 측정에서의 역할을 강조하였습니다.\n",
    "\n",
    "However, existing rPPG research primarily relies on manually extracted image and signal processing features, and few have used deep learning to minimize manual processing and enhance noise resilience.\n",
    "\n",
    "- 그러나 기존의 rPPG 연구는 주로 수동으로 추출된 이미지와 신호 처리 특징에 의존하며, 딥러닝을 사용한 연구는 거의 없었습니다.\n",
    "\n",
    "This study aims to address these gaps by training a deep neural network on a dataset containing various real-world scenarios, enhancing the robustness and applicability of the rPPG system in everyday environments.\n",
    "\n",
    "- 이 연구는 다양한 실제 세계 시나리오를 포함하는 데이터 세트에서 심층 신경망을 훈련시켜 rPPG 시스템의 견고성과 일상 환경에서의 적용 가능성을 향상시키는 것을 목표로 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5bdff",
   "metadata": {},
   "source": [
    "## Research method\n",
    "\n",
    "This study proposes a system capable of real-time heartbeat measurement using only facial video footage collected from smartphone cameras, without additional hardware devices.\n",
    "\n",
    "- 본 논문에서는 추가적인 하드웨어 장치 없이 오직 스마트폰 카메라에서 수집된 얼굴 영상을 사용해 실시간 심박 측정이 가능한 시스템을 제안합니다. \n",
    "\n",
    "Fig.1 shows the configuration of the proposed system, which is divided into two parts: building the learning model (Fig. 1(a)) and estimating the heartbeat by mounting the learned model on mobile, and then storing the results in a database (Fig. 1(b)).\n",
    "\n",
    "- Fig.1은 제안하는 시스템의 구성을 보여주며, 학습모델을 구축하는 부분(Fig. 1(a))과 학습된 모델을 모바일에 탑재해 심박을 추정하고 결과를 데이터베이스에 저장하는 부분(Fig. 1(b))으로 나뉩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./img/fig1.png\"><br>\n",
    "    Fig. 1. Flowchart for deep learning-based mobile application system. (a) Model training and exportation for deployment to the smartphone and (b) Inference on the smartphone in real-time using the trained model.<br>\n",
    "    그림 1. 딥러닝 기반 모바일 애플리케이션 시스템 흐름도. (a) 스마트폰에 배포하기 위한 모델 훈련 및 내보내기, (b) 훈련된 모델을 사용하여 스마트폰에서 실시간 추론.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3D Convolutional Neural Network\n",
    "\n",
    "The structure of the PhysNet[16] model was borrowed to train facial video footage. This model is based on a 3D convolutional neural network.\n",
    "\n",
    "- 얼굴 비디오 영상을 학습하기 위해 PhysNet[16] 모델의 구조를 차용하였습니다. 해당 모델은 3D 컨볼루션 신경망을 기반으로 합니다. \n",
    "\n",
    "For video composed of multiple image frames, a 3D convolutional neural network is used to effectively extract temporal features that exist between continuous frames.\n",
    "\n",
    "- 여러 이미지 프레임으로 구성된 비디오의 경우에는 연속되는 프레임 사이에 존재하는 시간적 특징을 효과적으로 추출하기 위해 3D 컨볼루션 신경망을 사용합니다.\n",
    "\n",
    "In this study, the PhysNet model, based on a 3D convolutional neural network, was used to design the learning model for extracting the rPPG signal, a time-series information, from facial video.\n",
    "\n",
    "- 본 연구에서는 얼굴 비디오에서 시계열 정보인 rPPG 신호를 추출하는 PhysNet 모델의 기본 구조를 사용하여 학습모델을 설계하였습니다.\n",
    "\n",
    "A pre-design was necessary to reduce the amount of operation as the model is operated in a CPU environment, not a GPU, where the amount of operation is largely influenced by frame length.\n",
    "\n",
    "- 학습모델은 GPU가 아닌 CPU 환경에서 구동되므로 연산량을 줄이기 위한 사전 설계가 필요합니다.\n",
    "\n",
    "The total frame length (T) was set to 90, considering the minimum necessary frames for estimating rPPG from facial video and ensuring no significant drop in performance with frames less than 90.\n",
    "\n",
    "- 본 연구에서는 얼굴 비디오에서 rPPG를 추정하는데에 필요한 최소한의 프레임을 설정하고자 프레임 길이를 변경하며 성능을 확인하였고, 실험 결과 프레임 길이가 90 프레임보다 작아질 경우 성능이 크게 하락해 전체 프레임 길이(T)는 90으로 설정하였습니다.\n",
    "\n",
    "Each frame was also resized to 80×80 to capture skin color changes at a resolution level.\n",
    "\n",
    "- 각 프레임의 크기 또한 피부색 변화가 포착되는 수준까지의 해상도를 고려해 80×80로 조정하였습니다.\n",
    "\n",
    "Finally, the model input was composed of a 5-dimensional input tensor of 4×3×90×80×80 (batch_size×channels×T×height×width), considering the limited mobile environment in the model building stage.\n",
    "\n",
    "- 최종적으로 모델의 입력은 4×3×90×80×80(batch_size×channels×T×height×width)의 5차원 입력 텐서로 구성되어 제한된 모바일 환경에서도 연산이 가능하도록 모델 구축 단계에서 고려해 주었습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/fig2.png\" /><br>\n",
    "    <div>Fig. 2. Architecture of the PhysNet-3DCNN.<br> 그림 2. PhysNet-3DCNN의 아키텍처.</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3D Convolutional Neural Network Training\n",
    "\n",
    "The dataset used for model training is VIPL-HR[27]. VIPL-HR is a dataset that includes approximately 30-second-long facial videos and biometric signals (SpO2, BVP, HR) recorded under various experimental conditions for 107 subjects.\n",
    "\n",
    "- 모델 학습을 위해 사용된 데이터셋은 VIPL-HR[27]입니다. VIPL-HR은 107명의 피험자에 대해 다양한 실험 조건에서 촬영된 약 30초 길이의 얼굴 비디오와 생체 신호(SpO2, BVP, HR)가 함께 기록된 데이터셋입니다.\n",
    "\n",
    "The recorded conditions include stable, motion, speaking, dark, bright, long-distance, and exercise scenarios. The recording devices used include webcams and smartphones, providing high similarity to the actual environment aimed in this study.\n",
    "\n",
    "- 실험 조건에는 안정적인 시나리오, 모션 시나리오, 말하는 시나리오, 어두운 시나리오, 밝은 시나리오, 장거리 시나리오, 운동 시나리오로 총 9가지의 시나리오에서 데이터가 기록되어있습니다.\n",
    "\n",
    "A total of 2378 video data were secured, with 1903 used for training data and 475 used for test data.\n",
    "\n",
    "- 이렇게 확보된 데이터셋은 총 2378개의 영상 데이터로, 1903개는 학습 데이터, 475개는 테스트 데이터로 사용되었습니다.\n",
    "\n",
    "In the learning process, a suitable loss function was set to minimize the difference between the rPPG signal estimated from facial videos and the actual heartbeat signal, BVP (blood volume pressure).\n",
    "\n",
    "- 학습 과정에서는 얼굴 영상에서 추정되는 rPPG 신호와 실제 심박 신호인 BVP(blood volume pressure) 신호의 차이를 최소화하는 데에 적합한 손실함수가 설정되어야 하므로 손실 함수는 변수 간 선형관계를 나타내는 대표적인 수치인 피어슨 상관계수를 활용하였습니다.\n",
    "\n",
    "The loss function was set as the negative Pearson correlation coefficient (1), subtracted from 1, to approach a loss close to 0 during the learning process.\n",
    "\n",
    "- 손실 함수는 1에서 피어슨 상관계수를 뺀 음의 피어슨 상관 계수(1)로 설정하여 학습과정 동안 손실이 0에 가까워지도록 하였습니다.\n",
    "\n",
    "Other learning parameters include a batch size of 4, a learning rate of 0.0001, and the use of the Adam optimizer. The learning was early stopped after 25 epochs as the validation loss did not fall below 0.23.\n",
    "\n",
    "- 이외 학습 파라미터로 배치 크기는 4, 학습률은 0.0001, 최적화 함수는 Adam optimizer를 사용하였고, 25 에포크 이후엔 검증 손실값이 0.23에서 떨어지지 않아 학습을 조기종료하였습니다.\n",
    "\n",
    "$$\n",
    "Loss = 1 - \\frac{T\\displaystyle\\sum_{i=1}^{T}x_iy_i-\\displaystyle\\sum_{i=1}^{T}x_i\\displaystyle\\sum_{i=1}^{T}y_i}{\\sqrt{(T\\displaystyle\\sum_{i=1}^{T}x_{i}^{2}-(\\displaystyle\\sum_{i=1}^{T}x_i)^2)(T\\displaystyle\\sum_{i=1}^{T}y_{i}^{2}-(\\displaystyle\\sum_{i=1}^{T}y_i)^2)}}\n",
    "$$\n",
    "<div align=\"right\">(1)</div>\n",
    "\n",
    "In the equation (1), \\(T\\) represents the total signal length or frame length, \\(i\\) sequentially refers to each video frame, \\(x\\) is the model-predicted rPPG signal, and \\(x_i\\) represents the rPPG signal for each frame. Similarly, \\(y\\) is the ground truth BVP signal, and \\(y_i\\) denotes the BVP signal for each frame.\n",
    "\n",
    "- 식 (1)에서 T는 전체 신호의 길이인 프레임 길이를 나타내며, \\(i\\)는 각 비디오 프레임을 순차적으로 가리킵니다. \\(x\\)는 모델 예측 결과인 rPPG 신호이고, \\(x_i\\)는 각 프레임에 대한 rPPG 신호를 나타냅니다. 마찬가지 로 \\(y\\)는 ground truth 값인 BVP 신호이고, \\(y_i\\)는 각 프레임에 대한 BVP 신호를 의미합니다.\n",
    "\n",
    "Finally, the trained model parameters were saved for operations within mobile, and the extracted model was set to inference mode, disabling dropout and ensuring that batch normalization does not run, using the stored variance and standard deviation from training.\n",
    "\n",
    "- 최종적으로 학습된 모델의 매개변수는 모바일 내에서의 연산을 위해 저장되어 추출되고, 추출된 모델은 학습 모드가 아닌 추론 모드로 설정해주었습니다.\n",
    "\n",
    "The model, converted to inference mode, was loaded onto the mobile project to operate as a module for heartbeat measurement.\n",
    "\n",
    "- 추론 모드로 변환된 모델은 모바일 프로젝트에 탑재되어 심박수 측정을 위한 모듈로써 동작하게 됩니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Implementation of a deep learning-based heart rate measurement mobile application\n",
    "- `딥러닝 기반 심박수 측정 모바일 애플리케이션 구현`\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/fig3.png\" /><br>\n",
    "    <div>Fig. 3. Mobile application process.<br> Fig. 3. 모바일 애플리케이션 과정.</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "We implemented a mobile application using Android so that the model learned on the smartphone can receive data in real time and make inferences.\n",
    "\n",
    "- 학습된 모델이 스마트폰에서 실시간으로 데이터를 받아 추론할 수 있도록 안드로이드를 이용해 모바일 애플리케이션을 구현하였습니다.\n",
    "\n",
    "The operation flow of the application is shown in Fig. Same as 3.\n",
    "\n",
    "- 애플리케이션의 동작 흐름은 Fig. 3과 같습니다. \n",
    "\n",
    "Facial images captured using the smartphone's front camera are used as input to the model, and the calculated heart rate is stored in the device's internal database using SQLite.\n",
    "\n",
    "- 스마트폰의 전면 카메라를 이용하여 촬영한 얼굴 영상이 모델의 입력으로 사용되며, 계산된 심박수는 SQLite를 사용하여 디바이스 내부 데이터베이스에 저장됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/fig4.png\" /><br>\n",
    "    <div>Fig. 4. Screen shots of the mobile application: (a) requiring user information, (b) capturing face images, and (c) displaying heart rate prediction result.<br> Fig. 4. 모바일 애플리케이션 스크린샷: (a) 사용자 정보 입력, (b) 얼굴 이미지 캡처, (c) 심박수 예측 결과 표시</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "When running the mobile application, the user enters his/her name, age, gender, and current status as shown in Fig. Enter in 4(a).\n",
    "\n",
    "- 모바일 애플리케이션을 실행하면, 사용자는 이름, 나이, 성별, 그리고 현재 상태를 Fig. 4(a)에서 입력합니다. \n",
    "\n",
    "The user information entered is stored in a database to calculate and provide personalized heart rate zones.\n",
    "\n",
    "- 입력된 사용자 정보는 개인 맞춤 심박 구간을 계산하고 제공하기 위해 데이터베이스에 저장됩니다. \n",
    "\n",
    "When measuring heart rate, the front camera operates and receives 90 image frames containing the user's face as input to estimate heart rate (Fig. 4(b)).\n",
    "\n",
    "- 심박수 측정 시, 전면 카메라가 작동되어 사용자의 얼굴이 포함된 90개의 이미지 프레임을 입력으로 받아 심박수를 추정합니다(Fig.4(b)). \n",
    "\n",
    "At this time, GoogleML Toolkit was used for face recognition in real-time video, and guidelines were presented on the camera screen to increase user convenience and accuracy of measurement results.\n",
    "\n",
    "- 이 때, 실시간 영상에서의 얼굴 인식을 위해 GoogleML Toolkit을 사용하였고, 사용자의 편의성과 측정 결과의 정확성을 높이기 위해 카메라 화면에 가이드라인을 제시하였습니다.\n",
    "\n",
    "The internal processes regarding tensor transformation and model input are performed using the pytorch_android_torchvision library.\n",
    "\n",
    "- 텐서 변환 및 모델 입력에 관한 내부 프로세스는 pytorch_android_torchvision 라이브러리를 이용해 수행됩니다. \n",
    "\n",
    "The tensor created within Android is in a 4-dimensional format, but the input of the PhysNet model installed on mobile is a 5-dimensional tensor, so conversion is required.\n",
    "\n",
    "- 안드로이드 내에서 생성된 텐서는 4차원 형식이지만, 모바일에 탑재된 PhysNet 모델의 입력은 5차원 텐서이므로 변환이 필요합니다. \n",
    "\n",
    "This conversion was performed using a separate Python script, as PyTorch Mobile does not support 5-dimensional tensor creation.\n",
    "\n",
    "- 이 변환은 파이토치 모바일이 5차원 텐서 생성을 지원하지 않기 때문에, 별도의 파이썬 스크립트를 사용하여 수행되었습니다.\n",
    "\n",
    "The measured rPPG signal detects peaks and calculates PPI. This PPI is used to calculate heart rate (HR), which is measured three times and recorded as the average value.\n",
    "\n",
    "- 측정된 rPPG 신호는 피크를 검출하여 PPI를 계산합니다. 이 PPI는 심박수(HR)를 계산하는 데 사용되며, 심박수는 3번 측정된 후 평균 값으로 기록됩니다. \n",
    "\n",
    "The final heart rate is stored in the device's internal database along with user information, and is shown in Fig. This is output in 4(c).\n",
    "\n",
    "- 최종 심박수는 사용자 정보와 함께 디바이스 내부 데이터베이스에 저장되며, Fig. 4(c)에서 출력됩니다.\n",
    "\n",
    "$$\n",
    "{HR}[bpm]=\\frac{60}{(peak(i+1)-peak(i))}\n",
    "$$\n",
    "<div align=\"right\">(2)</div>\n",
    "\n",
    "When heart rate measurement is completed, the screen automatically switches to the heart rate measurement record search screen (Fig. 4(c)).\n",
    "\n",
    "- 심박수 측정이 완료되면 자동으로 심박수 측정 기록 조회 화면으로 전환됩니다(Fig. 4(c)). \n",
    "\n",
    "Users can check their heart rate measurement history, including their heart rate, status, and measurement date.\n",
    "\n",
    "- 사용자는 자신의 심박수, 상태, 측정 날짜 등의 심박수 측정 기록을 확인할 수 있습니다.\n",
    "\n",
    "Finally, specialized heart rate zones are provided for each individual. This allows users to determine whether their calculated heart rate is in the normal range and their health status.\n",
    "\n",
    "- 최종적으로, 각 개인에 대해 특화된 심박수 구간이 제공됩니다. 이를 통해 사용자는 계산된 심박수가 정상 범위에 있는지, 그리고 그들의 건강 상태를 판단할 수 있습니다. \n",
    "\n",
    "Heart rate zones are calculated based on the user's age, gender, and condition, and resting heart rate zones and activity (exercise) heart rate zones use the standards recommended by the U.S. Centers for Disease Control and Prevention and the American College of Sports Medicine (ACSM), respectively.\n",
    "\n",
    "- 심박수 구간은 사용자의 나이, 성별, 상태를 기반으로 계산되며, 안정 시 심박수 구간과 활동(운동) 시 심박수 구간은 각각 미국 질병 통제 예방센터와 미국 스포츠 의학회(ACSM)의 권장 기준을 사용합니다.\n",
    "\n",
    "$$\n",
    "{HR}_{max}=220-age\n",
    "$$\n",
    "<div align=\"right\">(3)</div>\n",
    "\n",
    "$age$ represents the user's age, and ${HR}_{max}$ represents the maximum heart rate.\n",
    "\n",
    "- $age$는 사용자의 나이를, ${HR}_{max}$는 최대 심박수를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f8bcd",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "To verify the efficiency of the system proposed in this study, experiments were conducted under four experimental conditions (no movement and no lighting change, hand tremor, head movement, and lighting change).\n",
    "\n",
    "- 본 연구에서 제안하는 시스템의 효율성을 검증하기 위해 4가지 실험 조건(움직임과 조명 변화가 없는 상황, 손 떨림, 머리 움직임, 조명 변화)에서 실험을 진행하였습니다. \n",
    "\n",
    "The equipment and settings used in the experiment included the Galaxy A51 5G smartphone and CMS50D-BT pulse oximeter.\n",
    "\n",
    "- 실험에 사용된 장비 및 설정은 갤럭시 A51 5G 스마트폰과 CMS50D-BT 맥박 산소포화도 측정기를 포함합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/fig5.png\" /><br>\n",
    "    <div>Fig. 5. System evaluation environment; heart-rate measurement of the proposed system and pulse oximeter.<br> 그림 5. 시스템 평가 환경; 제안된 시스템과 맥박산소측정기의 심박수 측정.</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Nine healthy adults aged 20 to 25 participated in the experiment. While sitting on a chair, they held a smartphone directly toward their face with one hand and wore a finger pulse oximeter with the other hand to simultaneously measure rPPG and PPG values.\n",
    "\n",
    "- 실험에는 20～25세의 9명의 건강한 성인이 참여하여 의자에 앉은 상태에서 한쪽 손으로는 직접 스마트폰을 얼굴을 향해 들고 다른 손에는 손가락 산소포화도 측정기를 착용해 rPPG와 PPG 값을 동시에 측정하였습니다. \n",
    "\n",
    "Subjects were tested in 3 different conditions in 4 different conditions: a situation in which there was no change in lighting in a static position (Baseline), a situation in which the hand was moving (M1), a situation in which the head was moving (M2), and a situation in which the ambient light was changing (L1). The experiment was conducted for several minutes.\n",
    "\n",
    "- 피험자는 4개의 서로 다른 조건(정적인 자세에서 조명 변화가 없는 상황(Baseline), 손을 움직이는 상황(M1), 머리를 움직이는 상황(M2), 주변 조명 빛이 변하는 상황(L1))에서 각각 3분 동안 실험을 진행했습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/Table1.png\" /><br>\n",
    "    <div>Table 1. Comparison of HR measurement between rPPG(proposed) and PPG(groundtruth).<br> 표 1. rPPG(제안)와 PPG(groundtruth)의 HR 측정 비교.</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Table 1 shows the average, minimum, and maximum heart rate values calculated from rPPG and PPG for all subjects in order, and it can be seen that the rPPG value measured from the face is slightly higher than the PPG value measured from the fingers.\n",
    "\n",
    "- Table 1에서는 전체 피험자의 차례로 rPPG와 PPG로부터 계산된 심박수의 평균, 최소, 최대값을 나타내며 얼굴로부터 측정되는 rPPG 값이 손가락으로부터 측정된 PPG 값보다 약간 높게 측정되는 것을 알 수 있습니다.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/fig6.png\" /><br>\n",
    "    <div>Fig. 6. Distribution of HR estimates with and without makeup.<br> 그림 6. 구성 유무에 따른 HR 추정치 분포.</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Fig. 6 is a scatter plot of heart rate obtained using facial PPG and fingertip PPG in the baseline environment, through which the distribution and linear relationship of the measured values were confirmed.\n",
    "\n",
    "- Fig. 6는 Baseline 환경에서 안면 PPG와 손가락 끝 PPG를 사용해 얻은 심박수에 대한 산점도로 이를 통해 측정값의 분포 및 선형관계를 확인하였습니다. \n",
    " \n",
    " Most data appears to be close to a positive linear relationship, but outliers that deviate from the linear relationship can also be identified.\n",
    " \n",
    "- 대부분의 데이터는 양의 선형관계에 가까운 것으로 보이나, 선형관계에서 벗어난 이상치도 확인할 수 있습니다.\n",
    "\n",
    " As a result of checking the subjects who showed outliers, most of the measurements of subjects wearing makeup showed outliers.\n",
    " \n",
    "- 이상치를 보인 피험자를 확인해 본 결과, 메이크업을 한 상태의 피험자들의 측정값들이 이상치를 보이는 경우가 대부분이었습니다. \n",
    " \n",
    " Makeup is known to have a negative effect on rPPG measurement because it makes it difficult to extract blood oxygen saturation from facial images. This study confirmed that rPPG performance is greatly influenced by the presence or absence of makeup.\n",
    " \n",
    "- 메이크업은 얼굴 영상에서 혈액 산소 포화도 추출을 어렵게 하기 때문에 rPPG 측정에 부정적인 영향을 미치는 것으로 알려져 있어 메이크업 유무에 따라 rPPG 성능이 크게 좌우된다는 것을 본 연구를 통해서도 확인할 수 있었습니다.\n",
    "\n",
    "Therefore, we additionally divided the system into a Makeup group and a No Makeup group according to the presence or absence of makeup, and performed correlation and RMSE analysis on the entire group to compare system performance in environments with and without factors that negatively affect PPG measurement.\n",
    "\n",
    "- 그래서 추가적으로 메이크업 유무에 따라 Makeup 그룹과 No Makeup 그룹으로 나누고, 전체 그룹에 대하여 상관관계와 RMSE 분석을 진행해 PPG 측정에 부정적인 영향을 끼치는 요인이 있는 환경과 없는 환경에서의 시스템 성능을 비교해 보았습니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/Table2.png\" /><br>\n",
    "    <div>Table 2. Evaluation under different noisy conditions: correlation coefficient and Root Mean Square Error(RMSE) between HR from rPPG and PPG.<br> 표 2. 다양한 잡음 조건에서의 평가: rPPG와 PPG의 HR 간의 상관 계수 및 RMSE(평균 제곱근 오차).</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Table 2 shows the correlation coefficient and RMSE of heart rate between rPPG and PPG under various experimental conditions. In the comparison between the makeup group and the no-makeup group, no correlation was found in the makeup group in almost all conditions.\n",
    "\n",
    "- Table 2는 다양한 실험 조건에서 rPPG와 PPG 사이의 심박수의 상관계수와 RMSE를 보여줍니다. 메이크업 그룹과 노 메이크업 그룹 간의 비교에서 메이크업 그룹에서는 거의 모든 조건에서 상관관계가 나타나지 않았습니다. \n",
    "\n",
    "On the other hand, the no-makeup group showed a high correlation in the static situation (baseline). As you can see here, makeup has a huge impact on rPPG measurements.\n",
    "\n",
    "- 반면 노 메이크업 그룹에서는 정적 상황(Baseline)에서 높은 상관관계를 보였습니다. 여기에서 볼 수 있듯이, 메이크업은 rPPG 측정에 큰 영향을 미칩니다.\n",
    "\n",
    "Additionally, the deviation of RMSE between various environmental conditions did not show significant differences. This means that the system performed reliably under a variety of conditions.\n",
    "\n",
    "- 또한, 다양한 환경 조건들 간의 RMSE의 편차는 큰 차이를 보이지 않았습니다. 이는 시스템이 다양한 조건에서 안정적인 성능을 보였음을 의미합니다. \n",
    "\n",
    "However, the average RMSE of the makeup group was higher than that of the no-makeup group, confirming that makeup reduces the accuracy of rPPG measurements.\n",
    "\n",
    "- 그러나 메이크업 그룹의 평균 RMSE가 노 메이크업 그룹보다 높았습니다, 이는 메이크업이 rPPG 측정의 정확성을 저하시키는 것을 확인합니다.\n",
    "\n",
    "Moreover, in Figs. 7 compares the RMSE results of several end-to-end models. Although these models were trained on different datasets, the proposed system was tested on data collected from smartphones.\n",
    "\n",
    "- 더욱이, Fig. 7은 여러 엔드투엔드 모델의 RMSE 결과를 비교한 것입니다. 이 모델들은 다른 데이터세트로 훈련되었지만, 제안된 시스템은 스마트폰에서 수집한 데이터로 테스트되었습니다. \n",
    "\n",
    "As a result, the proposed system showed lower RMSE than existing models, indicating high accuracy and reliability of the proposed system.\n",
    "\n",
    "- 결과적으로 제안된 시스템이 기존의 모델들보다 더 낮은 RMSE를 보였습니다, 이는 제안된 시스템의 높은 정확성과 신뢰성을 의미합니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/fig7.png\" /><br>\n",
    "    <div>Fig. 7. Comparison of RMSE results of end-to-end models trained with VIPL-HR dataset.<br> 그림 7. VIPL-HR 데이터세트로 훈련된 엔드투엔드 모델의 RMSE 결과 비교.</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "From the above results, it can be seen that the performance of the system proposed in this study is more affected by the lighting environment than by movement, and by the presence or absence of makeup rather than environmental conditions.\n",
    "\n",
    "- 위의 결과로부터 본 연구에서 제안하는 시스템의 성능은 움직임보다는 조명 환경에 영향을 많이 받고, 환경 조건보다는 메이크업 여부에 더 큰 영향을 받는다는 것을 알 수 있습니다. \n",
    "\n",
    "As a result, it can be seen that optimal heart rate measurement is possible only when movement and lighting changes are minimized and makeup is not applied while measuring facial images.\n",
    "\n",
    "- 결과적으로, 얼굴 영상을 측정하는 동안에는 가능한 한 움직임과 조명 변화는 최소화하고 메이크업을 하지 않은 상태여야 최적의 심박 측정이 가능함을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee97b77",
   "metadata": {},
   "source": [
    "### conclusion\n",
    "\n",
    "In this paper, we developed a mobile application that can measure heart rate in real time without a separate device or sensor using a 3D convolutional neural network model learned on a PC.\n",
    "\n",
    "- 본 논문에서는 PC에서 학습된 3D 컨볼루션 신경망 모델을 이용하여 별도의 장치나 센서 없이 심박수를 실시간으로 측정할 수 있는 모바일 애플리케이션을 개발하였습니다.\n",
    "\n",
    "It captures images of the user's face through the smartphone's front camera and calculates the heart rate using them.\n",
    "\n",
    "- 스마트폰 전면 카메라를 통해 사용자의 얼굴 영상을 캡처하고 이를 통해 심박수를 계산한다.\n",
    "\n",
    "In the evaluation using a finger pulse oximeter, the deviation of RMSE was found to be about 1.14, which shows that heart rate measurement in daily life is possible.\n",
    "\n",
    "- 손가락 산소포화도 측정기를 사용한 평가에서 RMSE의 편차는 약 1.14로 나타났으며, 이는 일상생활에서의 심박수 측정이 가능함을 보여준다.\n",
    "\n",
    "The system proposed in this study can measure heart rate non-contactly regardless of time and place, and is expected to be useful in the healthcare field, including the field of cardiovascular disease.\n",
    "\n",
    "- 본 연구에서 제안한 시스템은 시간과 장소에 구애받지 않고 심박수를 비접촉식으로 측정할 수 있어, 심혈관질환 분야를 포함한 헬스케어 분야에서 유용하게 활용될 것으로 기대된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb0407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
