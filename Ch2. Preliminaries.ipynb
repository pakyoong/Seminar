{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524941ac",
   "metadata": {},
   "source": [
    "# Dive into Deep Learning\n",
    "# Ch 2. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f83a7",
   "metadata": {},
   "source": [
    "## intro\n",
    "To prepare for your dive into deep learning, you will need a few survival skills\n",
    "\n",
    "    딥러닝에 대한 다이빙을 준비하려면 다음과 같은 몇 가지 생존 기술이 필요합니다.\n",
    "\n",
    "(i) techniques for storing and manipulating data\n",
    "\n",
    "    데이터 저장 및 조작 기술\n",
    "\n",
    "(ii) libraries for ingesting and preprocessing data from a variety of sources\n",
    "\n",
    "    다양한 소스의 데이터를 수집하고 전처리하기 위한 라이브러리\n",
    "\n",
    "(iii) knowledge of the basic linear algebraic operations that we apply to high-dimensional data elements\n",
    "\n",
    "    고차원 데이터 요소에 적용되는 기본 선형 대수 연산에 대한 지식\n",
    "\n",
    "(iv) just enough calculus to determine which direction to adjust each parameter in order to decrease the loss function\n",
    "\n",
    "    손실 함수를 줄이기 위해 각 매개변수를 조정할 방향을 결정하기에 충분한 미적분\n",
    "\n",
    "(v) the ability to automatically compute derivatives so that you can forget much of the calculus you just learned\n",
    "\n",
    "    방금 배운 미적분의 대부분을 잊어버릴 수 있도록 도함수를 자동으로 계산하는 기능\n",
    "\n",
    "(vi) some basic fluency in probability, our primary language for reasoning under uncertainty\n",
    "\n",
    "    불확실성 하에서 추론하기 위한 기본 언어인 확률의 기본적인 유창성\n",
    "\n",
    "(vii) some aptitude for finding answers in the official documentation when you get stuck.\n",
    "\n",
    "    문제가 생겼을 때 공식 문서에서 답을 찾는 능력."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef40fc8",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "To conduct any task, we require a method to store and manage data. Two significant steps with data include acquisition and in-computer processing. We primarily deal with $n$-dimensional arrays, or tensors. Familiarity with the NumPy scientific computing package will simplify this.\n",
    "\n",
    " 어떤 작업을 수행하려면 데이터를 저장하고 관리하는 방법이 필요합니다. 데이터와 관련하여 중요한 두 단계는 획득 및 컴퓨터 내부에서의 처리입니다. 우리는 주로 $n$-차원 배열 또는 텐서를 다룹니다. NumPy 과학 계산 패키지에 익숙하다면 이를 이해하는데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be5dba",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "\n",
    "Tensor elements are accessible through indexing, with indexing starting at 0. Negative indexing allows for position-based element access from the end of the list. Slicing can be used for accessing a range of indices (e.g., X[start:stop]), the returned value includes the first index (start) but not the last (stop). When only one index (or slice) is specified for a $k^\\mathrm{th}$ order tensor, it is applied along axis 0.\n",
    "\n",
    "텐서 요소는 인덱싱을 통해 접근할 수 있으며, 인덱싱은 0부터 시작합니다. 음의 인덱싱을 사용하면 리스트의 끝에 상대적인 위치를 기반으로 요소에 접근할 수 있습니다. 슬라이싱은 일련의 인덱스에 접근하는 데 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bafbb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12, dtype=torch.float32)\n",
    "X = x.reshape(3, 4)\n",
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5a2af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 17\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d92655",
   "metadata": {},
   "source": [
    "To assign multiple elements the same value, we apply the indexing on the left-hand side of the assignment operation.\n",
    "\n",
    "여러 요소에 동일한 값을 할당하려면, 할당 연산의 왼쪽에 인덱싱을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c932e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2aa30",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "With the knowledge of tensor construction, reading, and writing elements, we can proceed with manipulating them using mathematical operations. Elementwise operations apply a standard scalar operation to each element of a tensor. Unary operators like $e^x$ can be applied elementwise as well.\n",
    "\n",
    "텐서 구조, 읽기 및 쓰기 요소를 이해하게 되면, 우리는 수학적 연산을 사용하여 그것들을 조작하는 데 나아갈 수 있습니다. 요소별 연산은 텐서의 각 요소에 표준 스칼라 연산을 적용합니다. 단항 연산자들도 요소별로 적용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b870fa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n",
       "        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,\n",
       "         22026.4648,  59874.1406])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3588cd0",
   "metadata": {},
   "source": [
    "Unary operators such as torch.exp(x) can be applied elementwise as well. Binary scalar operators, which map pairs of real numbers to a single real number, can also be extended to vectors of the same shape. This operation can be thought of as lifting the scalar function to an elementwise vector operation. Standard arithmetic operators have been lifted to elementwise operations for identically-shaped tensors.\n",
    "\n",
    "다음과 같은 단항 연산자인 torch.exp(x)도 요소별로 적용될 수 있습니다. 또한 두 개의 실수를 하나의 실수로 매핑하는 이항 스칼라 연산자도 동일한 모양의 벡터로 확장될 수 있습니다. 이 작업은 스칼라 함수를 요소별 벡터 연산으로 '올리는' 것으로 생각할 수 있습니다. 표준 산술 연산자들은 동일한 모양의 텐서에 대한 요소별 연산으로 '올려졌습니다'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84cc362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16b92a",
   "metadata": {},
   "source": [
    "We can also concatenate multiple tensors together, forming a larger tensor. This process depends on the axis along which we concatenate.\n",
    "\n",
    "우리는 또한 여러 텐서를 연결하여 더 큰 텐서를 형성할 수 있습니다. 이 과정은 우리가 연결하는 축에 따라 달라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a984c9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afeb96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4082e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724cbe10",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Under certain conditions, we can perform elementwise binary operations on tensors of different shapes by using the broadcasting mechanism. Broadcasting works as follows: (i) expand one or both arrays by copying elements along axes with length 1 to make the shapes compatible; (ii) perform an elementwise operation on the resulting arrays.\n",
    "\n",
    "브로드캐스팅 메커니즘을 사용하면 형상이 다른 텐서에 대해 요소별 이진 연산을 수행할 수 있습니다. 브로드캐스팅은 다음과 같은 두 단계로 진행됩니다: (i) 길이가 1인 축을 따라 요소를 복사하여 하나 이상의 배열을 확장하여 두 텐서의 형상을 호환 가능하게 만듭니다. (ii) 결과로 나온 배열에 대해 elementwise 연산을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f093c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddd287",
   "metadata": {},
   "source": [
    "Since `a` and `b` are $3\\times1$ and $1\\times2$ matrices, respectively, their shapes do not match up. Broadcasting produces a larger $3\\times2$ matrix by replicating matrix `a` along the columns and matrix `b` along the rows before adding them elementwise.\n",
    "\n",
    "`a`와 `b`는 각각 $3\\times1$ 및 $1\\times2$ 행렬이므로 그 모양이 일치하지 않습니다. 브로드캐스팅은 행렬 `a`를 열을 따라 복제하고 행렬 `b`를 행을 따라 복제하여 더 큰 $3\\times2$ 행렬을 생성한 후 요소별로 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a13cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda76691",
   "metadata": {},
   "source": [
    "### Saving Memory\n",
    "\n",
    "In this example, when we execute Y = Y + X, a new memory allocation is performed for the result of Y + X. As a result, the variable Y is reassigned to point to this new memory location, indicated by a different id(Y). This behavior can be problematic for two reasons.\n",
    "\n",
    "예를 들어, Y = X + Y라고 작성하면, Y가 이전에 가리키던 텐서를 참조 해제하고 새롭게 할당된 메모리를 가리키도록 한다. id() 함수를 사용하여 객체의 정확한 메모리 주소를 얻을 수 있는 Python의 id() 함수를 사용하여 이 문제를 설명할 수 있다. Y = Y + X를 실행한 후에는 id(Y)가 다른 위치를 가리킨다는 것을 알 수 있다. 이는 Python이 먼저 Y + X를 평가하고 결과에 대해 새로운 메모리를 할당한 다음 Y를 이 새로운 메모리 위치로 지정하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e8cae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e9d80",
   "metadata": {},
   "source": [
    "First, frequent memory allocation can be inefficient, especially when dealing with large tensors and performing updates multiple times per second in machine learning applications. Ideally, we want to perform these updates in place to avoid unnecessary memory allocation.\n",
    "\n",
    "Second, if multiple variables point to the same parameters, not updating in place can lead to memory leaks or references to stale parameters. Therefore, it's important to update all the references consistently.\n",
    "\n",
    "\n",
    "이는 두 가지 이유로 원하지 않을 수 있다. 첫째로, 우리는 불필요하게 메모리를 계속해서 할당하는 것을 원하지 않는다. 머신 러닝에서는 종종 수백 메가바이트의 매개변수가 있고 이들을 초당 여러 번 업데이트한다. 가능하면 이러한 업데이트를 in-place로 수행하고 싶다. 둘째로, 여러 변수에서 동일한 매개변수를 가리킬 수 있다. In-place 업데이트하지 않으면 이러한 참조를 모두 업데이트해야 하며, 그렇지 않으면 메모리 누수가 발생하거나 오래된 매개변수를 참조할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc39418",
   "metadata": {},
   "source": [
    "Fortunately, performing in-place operations is straightforward in Python. We can assign the result of an operation to a pre-allocated array Y using slice notation: Y[:] = <expression>. This allows us to update the values of Y without changing its memory location. Here's an example:\n",
    "\n",
    "다행히도, in-place 연산을 수행하는 것은 쉽다. 미리 할당된 배열 Y에 연산의 결과를 할당하기 위해 슬라이스 표기법을 사용할 수 있다: Y[:] = <식>. 이 개념을 설명하기 위해, zeros_like를 사용하여 초기화한 후 텐서 Z의 값을 Y와 동일한 모양으로 덮어씌우는 예제를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52c798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 139959156107680\n",
      "id(Z): 139959156107680\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9159f",
   "metadata": {},
   "source": [
    "[**If the value of X is not reused in subsequent computations, we can further reduce the memory overhead by using X[:] = X + Y or X += Y.**]\n",
    "\n",
    "[**만약 X의 값이 이후의 계산에서 재사용되지 않는다면, X[:] = X + Y 또는 X += Y를 사용하여 연산의 메모리 오버헤드를 줄일 수도 있다.**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3adb082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b5ff7",
   "metadata": {},
   "source": [
    "In the final code snippet, we update the values of X in place using X += Y. This operation modifies X directly, avoiding the need for a new memory allocation if X is not reused later.\n",
    "\n",
    "By leveraging in-place operations and updating tensors directly, we can effectively save memory and optimize the computational efficiency of our code.\n",
    "\n",
    "위의 코드 스니펫에서는 X += Y를 사용하여 X의 값을 곧바로 업데이트하여 메모리 오버헤드를 줄인다. 이 작업은 X가 이후에 재사용되지 않는 경우 새로운 메모리 할당이 필요하지 않도록 한다.\n",
    "\n",
    "in-place 연산과 텐서를 직접적으로 업데이트하여 메모리를 효과적으로 절약하고 코드의 계산 효율성을 최적화할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0cd732",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "To apply deep learning in real-world scenarios, we often need to preprocess and extract valuable information from raw data. The pandas library provides powerful tools for data manipulation and preprocessing. Here, we will focus on loading CSV files and performing basic operations using pandas.\n",
    "\n",
    "실제 상황에서 딥러닝을 적용하기 위해서는 원시 데이터에서 가치 있는 정보를 추출하고 전처리하는 작업이 필요합니다. pandas 라이브러리는 데이터 조작과 전처리를 위한 강력한 도구를 제공합니다. 여기에서는 CSV 파일을 로드하고 pandas를 사용하여 기본 작업을 수행하는 방법에 초점을 맞출 것입니다.\n",
    "\n",
    "### Reading the Dataset\n",
    "\n",
    "One of the most common formats for storing tabular data is the Comma-Separated Values (CSV) format. Each line in a CSV file represents a record, and the fields within a record are separated by commas. To demonstrate how to load CSV files using pandas, we will create a sample dataset file called house_tiny.csv. This dataset contains information about homes, including the number of rooms, roof type, and price.\n",
    "\n",
    "탭ular(테이블 형태의) 데이터를 저장하는 가장 일반적인 형식 중 하나는 쉼표로 구분된 값 (CSV) 형식입니다. CSV 파일에서 각 줄은 레코드를 나타내며, 레코드 내의 필드는 쉼표로 구분됩니다. pandas를 사용하여 CSV 파일을 로드하는 방법을 보여주기 위해 house_tiny.csv라는 샘플 데이터셋 파일을 만들겠습니다. 이 데이터셋은 방의 수, 지붕 종류 및 가격과 같은 주택에 대한 정보를 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f90b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''NumRooms,RoofType,Price\n",
    "NA,NA,127500\n",
    "2,NA,106000\n",
    "4,Slate,178100\n",
    "NA,NA,140000''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "487c2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms RoofType   Price\n",
      "0       NaN      NaN  127500\n",
      "1       2.0      NaN  106000\n",
      "2       4.0    Slate  178100\n",
      "3       NaN      NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b09aa9",
   "metadata": {},
   "source": [
    "The read_csv function reads the CSV file and creates a DataFrame, which is a tabular data structure provided by pandas. We can then perform various operations on the DataFrame to preprocess the data further.\n",
    "\n",
    "read_csv 함수는 CSV 파일을 읽어와 DataFrame이라는 테이블 형태의 데이터 구조로 만듭니다. 이후에는 DataFrame에서 추가적인 데이터 전처리 작업을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a676b2",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In supervised learning, we train models to predict a target value based on input values. The first step in processing the dataset is to separate the columns for input and target values. This can be done by selecting columns by name or index using iloc.\n",
    "\n",
    "지도 학습에서는 입력 값에 기반하여 지정된 목표(target) 값을 예측하는 모델을 훈련합니다. 데이터셋을 처리하는 첫 번째 단계는 입력과 목표 값을 나타내는 열을 분리하는 것입니다. 열은 이름이나 정수 기반의 위치 인덱싱(iloc)을 사용하여 선택할 수 있습니다.\n",
    "\n",
    "Missing values are common in data science and are often represented as NA or empty entries. They can be handled through imputation or deletion. Imputation replaces missing values with estimates, while deletion removes rows or columns containing missing values.\n",
    "\n",
    "결측값은 데이터 과학에서 흔하게 발생하며, NA나 빈 엔트리로 표시될 수 있습니다. 결측값은 대체(imputation) 또는 삭제(deletion)를 통해 처리할 수 있습니다. 대체는 결측값을 추정치로 대체하는 것이고, 삭제는 결측값을 포함하는 행이나 열을 제거하는 것입니다.\n",
    "\n",
    "For categorical input fields, NaN can be treated as a separate category. For example, the RoofType column can be split into RoofType_Slate and RoofType_nan columns. Rows with Slate as the roof type will have RoofType_Slate set to 1 and RoofType_nan set to 0, while rows with missing RoofType values will have the opposite values.\n",
    "\n",
    "범주형 입력 필드의 경우, NaN은 별도의 범주로 처리할 수 있습니다. 예를 들어, RoofType 열은 RoofType_Slate와 RoofType_nan 열로 분리할 수 있습니다. Slate인 경우 RoofType_Slate는 1이 되고 RoofType_nan은 0이 됩니다. 반면, RoofType 값이 결측값인 행은 그 반대의 값을 가지게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "403601ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       NaN               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       NaN               0             1\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19c37e",
   "metadata": {},
   "source": [
    "For missing numerical values, a common approach is to replace them with the mean value of the corresponding column.\n",
    "\n",
    "숫자형 결측값의 경우, 일반적인 방법은 해당 열의 평균값으로 결측값을 대체하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb9a8507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       3.0               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       3.0               0             1\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497d8a9",
   "metadata": {},
   "source": [
    "Data preparation is an important step in training deep learning models, where missing values need to be handled appropriately.\n",
    "\n",
    "데이터 준비는 결측값을 적절히 처리해야 하는 딥러닝 모델 훈련의 중요한 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae5d58",
   "metadata": {},
   "source": [
    "### Conversion to the Tensor Format\n",
    "\n",
    "Now that [**all the entries in `inputs` and `targets` are numerical, we can load them into a tensor**]\n",
    "\n",
    "이제 입력 및 대상의 모든 항목이 숫자이므로 텐서에 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbceb8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 0., 1.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 1., 0.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(targets.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b9a3d",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Data Processing Complexity: Data may span multiple files and tables, making processing complex.\n",
    "\n",
    "데이터 처리의 복잡성: 데이터는 여러 파일과 테이블에 걸쳐 있을 수 있어 처리가 복잡해질 수 있습니다.\n",
    "\n",
    "Data Types Variety: Data comes in many forms such as text, images, and audio, not just categorical and numeric.\n",
    "\n",
    "다양한 데이터 유형: 데이터는 범주형과 수치형뿐만 아니라 텍스트, 이미지, 오디오 등 다양한 형태로 제공됩니다.\n",
    "\n",
    "Advanced Tools Requirement: Advanced tools and algorithms are necessary to prevent data processing from becoming a bottleneck in machine learning.\n",
    "\n",
    "고급 도구의 필요성: 데이터 처리가 머신 러닝에서 병목이 되지 않도록 하기 위해 고급 도구와 알고리즘이 필요합니다.\n",
    "\n",
    "Data Quality: Attention must be paid to real-world data issues like outliers, faulty measurements, and recording errors.\n",
    "\n",
    "데이터 품질: 이상치, 잘못된 측정값, 기록 오류 등과 같은 실제 데이터 문제에 주의를 기울여야 합니다.\n",
    "\n",
    "Data Visualization Tools: Tools like seaborn, Bokeh, and matplotlib aid in inspecting data and identifying potential problems.\n",
    "\n",
    "데이터 시각화 도구: seaborn, Bokeh, matplotlib과 같은 도구를 사용하여 데이터를 검사하고 잠재적인 문제를 식별할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea84f0",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "By now, we can load datasets into tensors and manipulate these tensors with basic mathematical operations. To start building sophisticated models, we will also need a few tools from linear algebra. \n",
    "\n",
    "이제 데이터 세트를 텐서에 로드하고 기본 수학 연산으로 이러한 텐서를 조작할 수 있습니다. 정교한 모델 구축을 시작하려면 선형 대수학의 몇 가지 도구도 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780eaef",
   "metadata": {},
   "source": [
    "### Scalars\n",
    "\n",
    "In mathematics, scalars are individual real numbers without direction. They are used in everyday computations, such as converting temperatures. Scalars are often denoted by lower-case letters and are considered members of the real number space, denoted by $\\mathbb{R}$. For instance, the variables in the equation $c = \\frac{5}{9}(f - 32)$ are all scalars. In programming, scalars can be implemented as tensors containing just one element.\n",
    "\n",
    "수학에서 스칼라는 방향을 가지지 않는 개별 실수를 의미합니다. 이들은 온도 변환 같은 일상적인 계산에 사용됩니다. 스칼라는 종종 소문자로 표현되며, 실수 공간인 $\\mathbb{R}$의 구성원으로 간주됩니다. 예를 들어, 방정식 $c = \\frac{5}{9}(f - 32)$의 변수들은 모두 스칼라입니다. 프로그래밍에서는 스칼라를 하나의 요소만을 포함하는 텐서로 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf659104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d050eab",
   "metadata": {},
   "source": [
    "Here, it's creating two scalar tensors x and y with the values 3.0 and 2.0 respectively. Then, it performs standard mathematical operations on these tensors:\n",
    "\n",
    "여기서는 값이 각각 3.0과 2.0인 두 개의 스칼라 텐서 x와 y를 생성합니다. 그런 다음 다음 텐서에서 표준 수학 연산을 수행합니다.\n",
    "\n",
    "When this code is run, the output is a tuple of four elements, each of which is a result of the corresponding operation: (5.0, 6.0, 1.5, 9.0). This indicates that x + y is 5.0, x * y is 6.0, x / y is 1.5, and x raised to the power y (or x**y) is 9.0.\n",
    "\n",
    "\n",
    "이 코드가 실행되면 출력은 4개 요소의 튜플이며 각 요소는 해당 작업의 결과입니다: (5.0, 6.0, 1.5, 9.0). 이는 x + y가 5.0, x * y가 6.0, x / y가 1.5이고 x의 y승(또는 x**y)이 9.0임을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe34af",
   "metadata": {},
   "source": [
    "### 벡터\n",
    "\n",
    "A vector is a tuple consisting of one or more scalar values. It can be denoted in various ways:\n",
    "\n",
    "벡터는 하나 이상의 스칼라 값으로 구성된 튜플입니다. 이는 다양한 방식으로 표현될 수 있습니다\n",
    "\n",
    "$v = (a_1, a_2, a_3) = [a_1, a_2, a_3]$ or \n",
    "$v = \\begin{pmatrix}a_1\\\\a_2\\\\a_3\\end{pmatrix} = \\begin{bmatrix}a_1\\\\a_2\\\\a_3\\end{bmatrix}$\n",
    "\n",
    "Vectors have distinct mathematical operations, including the dot product and the cross product. The dot product of two vectors, $u$ and $v$, produces a scalar and is computed as $u \\cdot v = |u||v|cos\\theta$, where $\\theta$ is the angle between the two vectors.\n",
    "\n",
    "벡터는 내적과 외적이라는 고유한 수학적 연산을 가지고 있습니다. 두 벡터 $u$와 $v$의 내적은 스칼라를 생성하며, 이는 $u \\cdot v = |u||v|cos\\theta$로 계산됩니다. 여기서 $\\theta$는 두 벡터 사이의 각입니다.\n",
    "\n",
    "On the other hand, the cross product, denoted as $u\\times v$, results in a new vector. Its direction is perpendicular to both $u$ and $v$, and its magnitude is $|u||v|sin\\theta$, signifying the area of the parallelogram with $u$ and $v$ as sides.\n",
    "\n",
    "반면에, 외적은 $u\\times v$로 표시되며, 새로운 벡터를 생성합니다. 그 방향은 $u$와 $v$ 모두에게 수직이며, 그 크기는 $|u||v|sin\\theta$로, 이는 $u$와 $v$가 변인 평행사변형의 넓이를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7669145e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432a13a",
   "metadata": {},
   "source": [
    "We can refer to an element of a vector by using a subscript. For example, $x_2$ denotes the second element of $\\mathbf{x}$. Since $x_2$ is a scalar, we do not bold it. By default, we visualize vectors by stacking their elements vertically.\n",
    "\n",
    "벡터의 요소는 첨자(subscript)를 사용하여 나타낼 수 있습니다. 예를 들어, $x_2$는 $\\mathbf{x}$의 두 번째 요소를 나타냅니다. $x_2$는 스칼라이므로 굵게 표시하지 않습니다. 기본적으로, 우리는 벡터를 요소를 세로로 쌓아서 시각화합니다.\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    "\n",
    "\n",
    "Here $x_1, \\ldots, x_n$ are elements of the vector. Later on, we will distinguish between such *column vectors* and *row vectors* whose elements are stacked horizontally. Recall that [**we access a tensor's elements via indexing.**]\n",
    "\n",
    "\n",
    "여기서 $x_1, \\ldots, x_n$은 벡터의 요소입니다. 나중에는 이러한 열 벡터와 가로로 쌓인 행 벡터를 구분할 것입니다. 기억해주세요. [**텐서의 요소에는 색인(indexing)을 통해 접근합니다.**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6861ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cb685",
   "metadata": {},
   "source": [
    "To indicate that a vector contains $n$ elements, we write $\\mathbf{x} \\in \\mathbb{R}^n$. Formally, we call $n$ the *dimensionality* of the vector. \n",
    "\n",
    "벡터가 $n$개의 요소를 포함한다는 것을 나타내기 위해 $\\mathbf{x} \\in \\mathbb{R}^n$와 같이 표기합니다. 형식적으로, 우리는 $n$을 벡터의 차원이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a22fab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4725d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddd32b",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "Matrices are 2nd-order tensors, denoted by bold capital letters (e.g., $\\mathbf{X}$, $\\mathbf{Y}$, $\\mathbf{Z}$). In code, matrices are represented as tensors with two axes. A matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ contains $m \\times n$ real-valued scalars arranged in $m$ rows and $n$ columns. If $m = n$, the matrix is called square.\n",
    "\n",
    "\n",
    "행렬은 2차 텐서로, 굵은 대문자로 표기됩니다 (예: $\\mathbf{X}$, $\\mathbf{Y}$, $\\mathbf{Z}$). 코드에서는 행렬을 두 개의 축을 가진 텐서로 표현합니다. 행렬 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$은 $m$개의 행과 $n$개의 열로 구성된 $m \\times n$개의 실수값을 포함합니다. 만약 $m = n$이라면, 해당 행렬은 \"정사각 행렬\"이라고 합니다.\n",
    "\n",
    "\n",
    "Individual elements in a matrix are referred to using subscripts for row and column indices. For example, $a_{ij}$ represents the value in the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column of matrix $\\mathbf{A}$.\n",
    "행렬의 개별 요소는 행과 열 인덱스의 하위 첨자(subscript)를 사용하여 표기합니다. 예를 들어, $a_{ij}$는 행렬 $\\mathbf{A}$의 $i$번째 행과 $j$번째 열에 위치한 값을 나타냅니다.\n",
    "\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "\n",
    "In code, a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is represented as a 2nd-order tensor with shape ($m$, $n$). To convert a tensor of size $m \\times n$ to a matrix, we can use the reshape function with the desired shape ($m$, $n$).\n",
    "\n",
    "\n",
    "코드에서 행렬 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$은 ($m$, $n$) 모양의 2차 텐서로 표현됩니다. $m \\times n$ 크기의 텐서를 행렬로 변환하기 위해서는 reshape 함수를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d95c7547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087dff3c",
   "metadata": {},
   "source": [
    "Sometimes, we want to flip the axes. When we exchange a matrix's rows and columns, the result is called its *transpose*. Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for all $i$ and $j$. Thus, the transpose of an $m \\times n$ matrix is an $n \\times m$ matrix:\n",
    "\n",
    "때로는 축을 뒤집고 싶을 때가 있습니다. 행렬의 행과 열을 교환하여 얻는 결과를 *전치(transpose)*라고 합니다. 수식적으로 행렬 $\\mathbf{A}$의 전치는 $\\mathbf{A}^\\top$로 표기하며, 만약 $\\mathbf{B} = \\mathbf{A}^\\top$이라면, 모든 $i$와 $j$에 대해 $b_{ij} = a_{ji}$입니다. 따라서 $m \\times n$ 행렬의 전치는 $n \\times m$ 행렬이 됩니다:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In code, we can access any (**matrix's transpose**) as follows:\n",
    "\n",
    "코드에서는 (행렬의 전치)를 다음과 같이 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "627f460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dca160",
   "metadata": {},
   "source": [
    "[**Symmetric matrices are the subset of square matrices that are equal to their own transposes: $\\mathbf{A} = \\mathbf{A}^\\top$.**] The following matrix is symmetric:\n",
    "\n",
    "[대칭 행렬은 자신의 전치와 같은 정사각 행렬의 하위 집합입니다: $\\mathbf{A} = \\mathbf{A}^\\top$.] 다음 행렬은 대칭 행렬입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b5c01bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116777d",
   "metadata": {},
   "source": [
    "Matrices are useful for representing datasets. Typically, rows correspond to individual records and columns correspond to distinct attributes.\n",
    "\n",
    "행렬은 데이터셋을 표현하는 데 유용합니다. 일반적으로 행은 개별 레코드를 나타내고 열은 서로 다른 속성을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afded59b",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are higher-order arrays that provide a way to describe extensions beyond scalars, vectors, and matrices. In machine learning, tensors are represented by software objects of the tensor class, allowing for arbitrary numbers of axes. Tensors are denoted by capital letters with a special font face and can be indexed using a similar mechanism as matrices.\n",
    "\n",
    "텐서는 스칼라, 벡터 및 행렬을 넘어서는 확장을 설명하는 데 사용되는 고차원 배열입니다. 머신 러닝에서 텐서는 텐서 클래스의 소프트웨어 객체로 표현되며 임의의 축 수를 지원합니다. 텐서는 특별한 글꼴로 대문자로 표시되며 행렬과 유사한 방식으로 인덱싱될 수 있습니다.\n",
    "\n",
    "Tensors become particularly important when working with images. Images are represented as 3rd-order tensors, with axes corresponding to height, width, and channels (such as red, green, and blue). Collections of images are represented by 4th-order tensors, with distinct images indexed along the first axis. Higher-order tensors are constructed by increasing the number of shape components.\n",
    "\n",
    "이미지 처리에서 텐서는 특히 중요합니다. 이미지는 3차원 텐서로 표현되며 높이, 너비 및 채널 (예: 빨강, 초록, 파랑)에 해당하는 축을 가지고 있습니다. 이미지의 모음은 4차원 텐서로 표현되며 각각의 이미지는 첫 번째 축을 따라 인덱싱됩니다. 고차원 텐서는 모양 구성 요소의 수를 증가시킴으로써 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ca3606c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b6f1f",
   "metadata": {},
   "source": [
    "### Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and higher-order tensors all have some handy properties. For example, lementwise operations produce outputs that have the same shape as their operands.\n",
    "\n",
    "스칼라, 벡터, 행렬 및 고차원 텐서는 유용한 몇 가지 특성을 갖고 있습니다. 예를 들어, 요소별 연산은 출력 결과가 피연산자와 동일한 형태를 갖습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1100c3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44016172",
   "metadata": {},
   "source": [
    "The [**elementwise product of two matrices is called their *Hadamard product***] (denoted $\\odot$). Below, we spell out the entries of the Hadamard product of two matrices $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$:\n",
    "\n",
    "스칼라, 벡터, 행렬 및 고차원 텐서는 모두 일부 유용한 특성을 갖고 있습니다. 예를 들어, 요소별 연산은 출력이 피연산자와 동일한 모양을 갖습니다.\n",
    "\n",
    "두 행렬의 요소별 곱셈을 Hadamard 곱셈이라고하며 (기호 $\\odot$로 표시됨), 아래에는 두 행렬 $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$의 Hadamard 곱셈의 항목을 구체적으로 나열합니다.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28798474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c76477",
   "metadata": {},
   "source": [
    "[Adding or multiplying a scalar and a tensor] produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar.\n",
    "\n",
    "스칼라와 텐서의 덧셈 또는 곱셈은 원래 텐서와 동일한 형태의 결과를 생성합니다. 여기서 텐서의 각 요소는 스칼라와 더해지거나 (또는 곱해지는) 값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7327fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67138221",
   "metadata": {},
   "source": [
    "## Matrix-Vector Products\n",
    "\n",
    "Now that we know how to calculate dot products, we can begin to understand the *product* between an $m \\times n$ matrix $\\mathbf{A}$ and an $n$-dimensional vector $\\mathbf{x}$. To start off, we visualize our matrix in terms of its row vectors\n",
    "\n",
    "이제 점곱을 계산하는 방법을 알았으므로, $m \\times n$ 행렬 $\\mathbf{A}$와 $n$ 차원 벡터 $\\mathbf{x}$ 사이의 곱셈을 이해할 수 있습니다. 시작하기 전에, 우리는 행 벡터로 표현된 행렬을 시각화합니다.\n",
    "\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$.\n",
    "\n",
    "여기서 각각의 $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$는 행렬 $\\mathbf{A}$의 $i^\\mathrm{th}$ 행을 나타내는 행 벡터입니다.\n",
    "\n",
    "[**The matrix-vector product $\\mathbf{A}\\mathbf{x}$ is simply a column vector of length $m$, whose $i^\\mathrm{th}$ element is the dot product $\\mathbf{a}^\\top_i \\mathbf{x}$:**]\n",
    "\n",
    "[행렬-벡터 곱셈 $\\mathbf{A}\\mathbf{x}$는 단순히 길이가 $m$인 열 벡터이며, 그 $i^\\mathrm{th}$ 요소는 $\\mathbf{a}^\\top_i \\mathbf{x}$의 점곱입니다:]\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can think of multiplication with a matrix $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ as a transformation that projects vectors from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$. These transformations are remarkably useful. For example, we can represent rotations as multiplications by certain square matrices.  Matrix-vector products also describe the key calculation involved in computing the outputs of each layer in a neural network given the outputs from the previous layer.\n",
    "\n",
    "우리는 행렬 $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$와의 곱셈을 $\\mathbb{R}^{n}$에서 $\\mathbb{R}^{m}$로 벡터를 투영하는 변환으로 생각할 수 있습니다. 이러한 변환은 매우 유용합니다. 예를 들어, 회전은 특정한 정사각형 행렬의 곱셈으로 표현할 수 있습니다. 행렬-벡터 곱셈은 또한 신경망의 각 계층에서 이전 계층의 출력을 통해 출력을 계산하는 주요 계산을 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86099f",
   "metadata": {},
   "source": [
    "To express a matrix-vector product in code, we use the `mv` function. Note that the column dimension of `A` (its length along axis 1) must be the same as the dimension of `x` (its length). PyTorch has a convenience operator `@` that can execute both matrix-vector and matrix-matrix products (depending on its arguments). Thus we can write `A@x`.\n",
    "\n",
    "코드에서 행렬-벡터 곱셈을 표현하기 위해 mv 함수를 사용합니다. 여기서 A의 열 차원 (축 1을 따라 길이)은 x의 차원과 동일해야 함에 주의하세요. PyTorch에는 편의를 위해 @ 연산자가 있으며, 이 연산자는 행렬-벡터 및 행렬-행렬 곱셈을 모두 실행할 수 있습니다 (인수에 따라 다름). 따라서 A@x와 같이 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79bfa8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = A.float()\n",
    "x = x.float()\n",
    "A.shape, x.shape, torch.mv(A, x), A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cda94",
   "metadata": {},
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "\n",
    "Say that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$와 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$ 두 행렬이 있다고 가정해봅시다:\n",
    "\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "Let $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ denote the row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$ and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ denote the column vector from the $j^\\mathrm{th}$ column of the matrix $\\mathbf{B}$:\n",
    "\n",
    "$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$는 행렬 $\\mathbf{A}$의 $i^\\mathrm{th}$ 행을 나타내는 행 벡터이고, \\mathbf{b}_{j} \\in \\mathbb{R}^k$는 행렬 $\\mathbf{B}$의 $j^\\mathrm{th}$ 열을 나타내는 열 벡터라고 합시다:\n",
    "\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "To form the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$, we simply compute each element $c_{ij}$ as the dot product between the $i^{\\mathrm{th}}$ row of $\\mathbf{A}$ and the $j^{\\mathrm{th}}$ column of $\\mathbf{B}$, i.e., $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "행렬 곱셈의 결과인 행렬 $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$을 형성하려면, $\\mathbf{A}$의 $i^{\\mathrm{th}}$ 행과 $\\mathbf{B}$의 $j^{\\mathrm{th}}$ 열 사이의 점곱인 각 요소 $c_{ij}$를 단순히 계산하면 됩니다. 즉, $\\mathbf{a}^\\top_i \\mathbf{b}_j$를 계산하면 됩니다.\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "[**We can think of the matrix-matrix multiplication $\\mathbf{AB}$ as performing $m$ matrix-vector products or $m \\times n$ dot products and stitching the results together to form an $n \\times m$ matrix.**] In the following snippet, we perform matrix multiplication on `A` and `B`. Here, `A` is a matrix with 2 rows and 3 columns, and `B` is a matrix with 3 rows and 4 columns. After multiplication, we obtain a matrix with 2 rows and 4 columns.\n",
    "\n",
    "[행렬-행렬 곱셈 $\\mathbf{AB}$를 생각할 때, $m$개의 행렬-벡터 곱셈 또는 $m \\times n$ 개의 점곱을 수행하고 그 결과를 합쳐 $n \\times m$ 행렬을 형성한다고 생각할 수 있습니다.] 다음 코드 스니펫에서는 A와 B에 대한 행렬 곱셈을 수행합니다. 여기서 A는 2행 3열의 행렬이고, B는 3행 4열의 행렬입니다. 곱셈 후에는 2행 4열의 행렬을 얻게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3690a264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "torch.mm(A, B), A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571de543",
   "metadata": {},
   "source": [
    "### Norms\n",
    "\n",
    "Some of the most useful operators in linear algebra are *norms*. Informally, the norm of a vector tells us how *big* it is. For instance, the $\\ell_2$ norm measures the (Euclidean) length of a vector. Here, we are employing a notion of *size* that concerns the magnitude of a vector's components (not its dimensionality). \n",
    "\n",
    "선형 대수학에서 가장 유용한 연산자 중 하나는 놈입니다. 비공식적으로, 벡터의 놈은 벡터가 얼마나 큰지를 알려줍니다. 예를 들어, $\\ell_2$ 놈은 벡터의 (유클리드) 길이를 측정합니다. 여기서는 벡터의 성분의 크기(차원 수가 아님)에 관한 크기 개념을 사용합니다.\n",
    "\n",
    "A norm is a function $\\| \\cdot \\|$ that maps a vector to a scalar and satisfies the following three properties:\n",
    "\n",
    "놈은 벡터를 스칼라에 매핑하는 함수 $| \\cdot |$이며, 다음 세 가지 속성을 만족합니다:\n",
    "\n",
    "1. Given any vector $\\mathbf{x}$, if we scale (all elements of) the vector by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n",
    "\n",
    "주어진 벡터 $\\mathbf{x}$에 대해, 벡터의 모든 요소를 스칼라 $\\alpha \\in \\mathbb{R}$로 스케일링하면, 그 놈도 그에 따라 스케일링됩니다:\n",
    "\n",
    "   $$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$$\n",
    "2. For any vectors $\\mathbf{x}$ and $\\mathbf{y}$:    norms satisfy the triangle inequality:\n",
    "\n",
    "벡터 $\\mathbf{x}$와 $\\mathbf{y}$에 대해, 놈은 삼각 부등식을 만족합니다:\n",
    "\n",
    "   $$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$$\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n",
    "\n",
    "벡터의 놈은 음이 아니며 벡터가 0일 때만 사라집니다:\n",
    "\n",
    "   $$\\|\\mathbf{x}\\| > 0 \\text{ for all } \\mathbf{x} \\neq 0.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Many functions are valid norms and different norms encode different notions of size. The Euclidean norm that we all learned in elementary school geometry when calculating the hypotenuse of right triangle is the square root of the sum of squares of a vector's elements. Formally, this is called [**the $\\ell_2$ *norm***] and expressed as\n",
    "\n",
    "많은 함수들이 유효한 놈이며, 서로 다른 놈은 서로 다른 크기 개념을 인코드합니다. 우리가 초등학교 기하학에서 직각 삼각형의 빗변을 계산할 때 배운 유클리드 놈은 벡터의 요소의 제곱의 합의 제곱근입니다. 공식적으로 이것은 [$\\ell_2$ 놈]이라고 부르며 다음과 같이 표현됩니다:\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$$**)\n",
    "\n",
    "The method `norm` calculates the $\\ell_2$ norm.\n",
    "\n",
    "norm 메소드는 $\\ell_2$ 놈을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a66ce35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ad55e",
   "metadata": {},
   "source": [
    "[**The $\\ell_1$ norm**] is also popular and the associated metric is called the Manhattan distance. By definition, the $\\ell_1$ norm sums the absolute values of a vector's elements:\n",
    "\n",
    "[$\\ell_1$ 놈]도 인기가 있으며, 관련 메트릭은 맨해튼 거리라고 합니다. 정의에 따르면, $\\ell_1$ 놈은 벡터의 요소의 절대값을 합산합니다:\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
    "\n",
    "Compared to the $\\ell_2$ norm, it is less sensitive to outliers. To compute the $\\ell_1$ norm, we compose the absolute value with the sum operation.\n",
    "\n",
    "$\\ell_2$ 놈에 비해 이상치에 대한 민감도가 낮습니다. $\\ell_1$ 놈을 계산하기 위해, 절대값과 합 연산을 결합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a67fd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4d9bf",
   "metadata": {},
   "source": [
    "## Calculus\n",
    "\n",
    "The ancient Greek mathematician Archimedes devised a method to calculate the area of a circle by inscribing polygons with increasing numbers of vertices inside the circle. As the number of vertices increases, the height of each triangle approaches the radius of the circle, and the base approaches $2\\pi r/n$. By multiplying the number of triangles by the average base and height, the area of the polygon approaches $\\pi r^2$, the area of the circle.\n",
    "\n",
    "고대 그리스 수학자 아르키메데스는 원 안에 점점 더 많은 꼭지점을 가진 다각형을 그려서 원의 넓이를 계산하는 방법을 개발했습니다. 꼭지점의 수가 증가함에 따라 각 삼각형의 높이는 원의 반지름에 접근하고, 밑변은 $2\\pi r/n$에 접근합니다. 삼각형의 수를 평균 밑변과 높이로 곱함으로써 다각형의 넓이는 $\\pi r^2$, 즉 원의 넓이에 접근합니다.\n",
    "\n",
    "![Finding the area of a circle as a limit procedure.](./img/polygon-circle.svg)\n",
    "\n",
    "This procedure led to the development of differential calculus and integral calculus. Differential calculus helps us understand how to change function values by manipulating their arguments, which is crucial for optimization problems in deep learning. Optimization involves updating parameters to minimize the loss function. However, our ultimate goal is generalization, which focuses on performing well on unseen data.\n",
    "\n",
    "\n",
    "이 절차는 미적분과 적분의 발전으로 이어졌습니다. 미적분은 함수의 값을 인자를 조작하여 증가시키거나 감소시키는 방법을 이해하는 데 도움을 줍니다. 이는 딥러닝에서 최적화 문제에 중요한 역할을 합니다. 최적화는 손실 함수를 최소화하기 위해 매개변수를 업데이트하는 작업을 포함합니다. 그러나 최종 목표는 보이지 않는 데이터에서의 성능을 향상시키는 일반화에 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4707efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb7d9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a19c7864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "for h in 10.0**np.arange(-1, -6, -1):\n",
    "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfaab4",
   "metadata": {},
   "source": [
    "There are several equivalent notational conventions for derivatives. Given $y = f(x)$, the following expressions are equivalent:\n",
    "\n",
    "도함수에 대한 여러 가지 등가한 표기법이 있습니다. $y = f(x)$라고 할 때, 다음 표현들은 동등합니다:\n",
    "\n",
    "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
    "\n",
    "where the symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators*. Below, we present the derivatives of some common functions:\n",
    "\n",
    "여기서 기호 $\\frac{d}{dx}$와 $D$는 미분 연산자입니다. 아래에서는 일부 일반적인 함수들의 도함수를 제시합니다\n",
    "\n",
    "$$\\begin{aligned} \\frac{d}{dx} C & = 0 && \\text{for any constant $C$} \\\\ \\frac{d}{dx} x^n & = n x^{n-1} && \\text{for } n \\neq 0 \\\\ \\frac{d}{dx} e^x & = e^x \\\\ \\frac{d}{dx} \\ln x & = x^{-1} \\end{aligned}$$\n",
    "\n",
    "Functions composed from differentiable functions are often themselves differentiable. The following rules come in handy for working with compositions of any differentiable functions $f$ and $g$, and constant $C$.\n",
    "\n",
    "미분 가능한 함수들로 구성된 함수들은 자체적으로도 미분 가능합니다. 다음 규칙들은 어떤 미분 가능한 함수 $f$와 $g$와 상수 $C$로 구성된 합성함수에 대해 유용합니다.\n",
    "\n",
    "$$\\begin{aligned} \\frac{d}{dx} [C f(x)] & = C \\frac{d}{dx} f(x) && \\text{Constant multiple rule} \\\\ \\frac{d}{dx} [f(x) + g(x)] & = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) && \\text{Sum rule} \\\\ \\frac{d}{dx} [f(x) g(x)] & = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) && \\text{Product rule} \\\\ \\frac{d}{dx} \\frac{f(x)}{g(x)} & = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} && \\text{Quotient rule} \\end{aligned}$$\n",
    "\n",
    "Using this, we can apply the rules to find the derivative of $3 x^2 - 4x$ via\n",
    "\n",
    "이를 사용하여 $3 x^2 - 4x$의 도함수를 구할 수 있습니다.\n",
    "\n",
    "$$\\frac{d}{dx} [3 x^2 - 4x] = 3 \\frac{d}{dx} x^2 - 4 \\frac{d}{dx} x = 6x - 4.$$\n",
    "\n",
    "Plugging in $x = 1$ shows that, indeed, the derivative is $2$ at this location. Note that derivatives tell us the *slope* of a function at a particular ocation.\n",
    "\n",
    "$x = 1$을 대입하면, 실제로 해당 위치에서 도함수가 $2$임을 알 수 있습니다. 미분은 특정 위치에서 함수의 기울기를 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05252c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt#@saved2ld2l.plt\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def use_svg_display():  #@save\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "    \n",
    "def set_figsize(figsize=(3.5, 2.5)):  #@save\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63433cf4",
   "metadata": {},
   "source": [
    "We need to define a few functions. As its name indicates, `use_svg_display` tells `matplotlib` to output graphics in SVG format for crisper images. The comment `#@save` is a special modifier that allows us to save any function, class, or other code block to the `d2l` package so that we can invoke it later without repeating the code, e.g., via `d2l.use_svg_display()`.\n",
    "\n",
    "일부 함수를 정의해야 합니다. use_svg_display라는 이름이 의미하는 대로 matplotlib을 사용하여 그래픽을 SVG 형식으로 출력하여 더 선명한 이미지를 얻을 수 있습니다. #@save 주석은 d2l 패키지에 있는 함수, 클래스 또는 다른 코드 블록을 저장하여 코드를 반복하지 않고 나중에 호출할 수 있도록 하는 특수한 수정자입니다. 예를 들어 d2l.use_svg_display()를 통해 호출할 수 있습니다.\n",
    "\n",
    "Conveniently, we can set figure sizes with `set_figsize`. Since the import statement `from matplotlib import pyplot as plt` was marked via `#@save` in the `d2l` package, we can call `d2l.plt`.\n",
    "\n",
    "set_figsize를 사용하여 그림 크기를 설정할 수 있습니다. from matplotlib import pyplot as plt와 같은 import 문은 d2l 패키지의 #@save로 표시되어 있으므로 d2l.plt를 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87605ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961077cd",
   "metadata": {},
   "source": [
    "The `set_axes` function can associate axes with properties, including labels, ranges, and scales.\n",
    "\n",
    "set_axes 함수는 레이블, 범위, 스케일과 같은 속성을 가진 축을 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cfb45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "\n",
    "    def has_one_axis(X):  # True if X (tensor or list) has 1 axis\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X): X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    if axes is None:\n",
    "        axes = d2l.plt.gca()\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525307c",
   "metadata": {},
   "source": [
    "With these three functions, we can define a `plot` function to overlay multiple curves. Much of the code here is just ensuring that the sizes and shapes of inputs match.\n",
    "\n",
    "이 세 가지 함수를 사용하여 여러 곡선을 겹쳐서 그릴 수 있는 plot 함수를 정의할 수 있습니다. 여기서 코드의 많은 부분은 입력의 크기와 모양이 일치하도록 하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c81bf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"183.35625pt\" viewBox=\"0 0 243.529359 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-06-28T16:00:52.085253</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 243.529359 183.35625 \n",
       "L 243.529359 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "L 40.603125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 49.480398 145.8 \n",
       "L 49.480398 7.2 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mbec4c53575\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbec4c53575\" x=\"49.480398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(46.299148 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 110.702968 145.8 \n",
       "L 110.702968 7.2 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbec4c53575\" x=\"110.702968\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(107.521718 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 171.925539 145.8 \n",
       "L 171.925539 7.2 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbec4c53575\" x=\"171.925539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(168.744289 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 233.148109 145.8 \n",
       "L 233.148109 7.2 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbec4c53575\" x=\"233.148109\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(229.966859 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(135.29375 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 40.603125 116.769994 \n",
       "L 235.903125 116.769994 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"mf6111b661a\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf6111b661a\" x=\"40.603125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.240625 120.569213) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 40.603125 78.886651 \n",
       "L 235.903125 78.886651 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf6111b661a\" x=\"40.603125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(27.240625 82.685869) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 40.603125 41.003307 \n",
       "L 235.903125 41.003307 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf6111b661a\" x=\"40.603125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 44.802526) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798437 85.121094) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 49.480398 116.769994 \n",
       "L 55.602655 119.573361 \n",
       "L 61.724912 121.922129 \n",
       "L 67.847169 123.816296 \n",
       "L 73.969426 125.255863 \n",
       "L 80.091683 126.24083 \n",
       "L 86.21394 126.771197 \n",
       "L 92.336197 126.846963 \n",
       "L 98.458454 126.46813 \n",
       "L 104.580711 125.634696 \n",
       "L 110.702968 124.346663 \n",
       "L 116.825225 122.604029 \n",
       "L 122.947482 120.406795 \n",
       "L 129.069739 117.754961 \n",
       "L 135.191996 114.648527 \n",
       "L 141.314254 111.087492 \n",
       "L 147.436511 107.071858 \n",
       "L 153.558768 102.601624 \n",
       "L 159.681025 97.676789 \n",
       "L 165.803282 92.297354 \n",
       "L 171.925539 86.463319 \n",
       "L 178.047796 80.174684 \n",
       "L 184.170053 73.431449 \n",
       "L 190.29231 66.233614 \n",
       "L 196.414567 58.581179 \n",
       "L 202.536824 50.474143 \n",
       "L 208.659081 41.912508 \n",
       "L 214.781338 32.896272 \n",
       "L 220.903595 23.425436 \n",
       "L 227.025852 13.5 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 49.480398 139.5 \n",
       "L 55.602655 137.984666 \n",
       "L 61.724912 136.469333 \n",
       "L 67.847169 134.953999 \n",
       "L 73.969426 133.438665 \n",
       "L 80.091683 131.923331 \n",
       "L 86.21394 130.407998 \n",
       "L 92.336197 128.892664 \n",
       "L 98.458454 127.37733 \n",
       "L 104.580711 125.861996 \n",
       "L 110.702968 124.346663 \n",
       "L 116.825225 122.831329 \n",
       "L 122.947482 121.315995 \n",
       "L 129.069739 119.800661 \n",
       "L 135.191996 118.285328 \n",
       "L 141.314254 116.769994 \n",
       "L 147.436511 115.25466 \n",
       "L 153.558768 113.739327 \n",
       "L 159.681025 112.223993 \n",
       "L 165.803282 110.708659 \n",
       "L 171.925539 109.193325 \n",
       "L 178.047796 107.677992 \n",
       "L 184.170053 106.162658 \n",
       "L 190.29231 104.647324 \n",
       "L 196.414567 103.13199 \n",
       "L 202.536824 101.616657 \n",
       "L 208.659081 100.101323 \n",
       "L 214.781338 98.585989 \n",
       "L 220.903595 97.070655 \n",
       "L 227.025852 95.555322 \n",
       "\" clip-path=\"url(#pdbd402c005)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 40.603125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 7.2 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 47.603125 44.55625 \n",
       "L 172.153125 44.55625 \n",
       "Q 174.153125 44.55625 174.153125 42.55625 \n",
       "L 174.153125 14.2 \n",
       "Q 174.153125 12.2 172.153125 12.2 \n",
       "L 47.603125 12.2 \n",
       "Q 45.603125 12.2 45.603125 14.2 \n",
       "L 45.603125 42.55625 \n",
       "Q 45.603125 44.55625 47.603125 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 49.603125 20.298438 \n",
       "L 59.603125 20.298438 \n",
       "L 69.603125 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(77.603125 23.798438) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 49.603125 34.976562 \n",
       "L 59.603125 34.976562 \n",
       "L 69.603125 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Tangent line (x=1) -->\n",
       "     <g transform=\"translate(77.603125 38.476562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \n",
       "L 4684 2906 \n",
       "L 4684 2381 \n",
       "L 678 2381 \n",
       "L 678 2906 \n",
       "z\n",
       "M 678 1631 \n",
       "L 4684 1631 \n",
       "L 4684 1100 \n",
       "L 678 1100 \n",
       "L 678 1631 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdbd402c005\">\n",
       "   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc4fac",
   "metadata": {},
   "source": [
    "Now we can [**plot the function $u = f(x)$ and its tangent line $y = 2x - 3$ at $x=1$**], where the coefficient $2$ is the slope of the tangent line.\n",
    "\n",
    "이제 함수 $u = f(x)$와 $y = 2x - 3$의 $x=1$에서의 접선을 그릴 수 있습니다. 여기서 $2$는 접선의 기울기입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bf739",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Fortunately all modern deep learning frameworks take this work off of our plates by offering *automatic differentiation* (often shortened to *autograd*). As we pass data through each successive function, the framework builds a *computational graph*  that tracks how each value depends on others. To calculate derivatives, automatic differentiation works backwards through this graph applying the chain rule. The computational algorithm for applying the chain rule in this fashion is called *backpropagation*.\n",
    "\n",
    "\n",
    "다행히도 모든 최신 딥러닝 프레임워크는 미분 작업을 자동화하기 위해 자동 미분 (자주 autograd로 약칭됨)을 제공합니다. 각 함수를 통과할 때마다 프레임워크는 각 값이 다른 값에 어떻게 의존하는지 추적하는 계산 그래프를 구성합니다. 도함수를 계산하기 위해 자동 미분은 이 그래프를 역방향으로 따라가며 연쇄 법칙을 적용합니다. 이러한 방식으로 연쇄 법칙을 적용하는 계산 알고리즘을 역전파라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fe4c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and set requires_grad=True to track computation with it\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Define a function\n",
    "y = x ** 2\n",
    "\n",
    "# Backward propagation to compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82b26c",
   "metadata": {},
   "source": [
    "We first import the torch module.\n",
    "\n",
    "먼저 torch 모듈을 가져옵니다.\n",
    "\n",
    "We then create a tensor x with an initial value of 3. The requires_grad=True argument tells PyTorch that we want to compute gradients with respect to this tensor during the backward pass.\n",
    "\n",
    "그런 다음 초기 값이 3인 tensor x를 생성합니다. requires_grad=True 인수는 backward pass 동안 이 tensor에 대한 gradient를 계산하려고 함을 PyTorch에 알립니다.\n",
    "\n",
    "Next, we define a function y = x ** 2.\n",
    "\n",
    "다음으로, 함수 y = x ** 2를 정의합니다.\n",
    "\n",
    "We then call y.backward(), which computes the gradient of y with respect to all of its dependencies (in this case x) using the chain rule.\n",
    "\n",
    "그런 다음 y.backward()를 호출하여 chain rule을 사용하여 y의 모든 종속성 (이 경우 x)에 대한 y의 gradient를 계산합니다.\n",
    "\n",
    "Finally, we print x.grad, which is the gradient of y at x=3. In this case, since y=x**2, its derivative is 2*x, so at x=3, the gradient should be 2*3=6.\n",
    "\n",
    "마지막으로 x.grad를 출력합니다. 이것은 x=3에서 y의 gradient입니다. 이 경우, y=x**2이므로 미분값은 2*x이므로, x=3에서 gradient는 2*3=6이어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ade3968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z with respect to x: tensor([7.])\n",
      "Gradient of z with respect to y: tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Define a function\n",
    "z = x * y + x * x\n",
    "\n",
    "# Backward propagation to compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradient of z with respect to x:\", x.grad)\n",
    "print(\"Gradient of z with respect to y:\", y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4d6d4",
   "metadata": {},
   "source": [
    "Import the torch module.\n",
    "\n",
    "torch 모듈을 가져옵니다.\n",
    "\n",
    "Create two tensors x and y with initial values 2 and 3 respectively. requires_grad=True tells PyTorch that we want to compute gradients with these tensors during the backward pass.\n",
    "\n",
    "초기 값이 각각 2와 3인 tensor x와 y를 생성합니다. requires_grad=True는 backward pass 동안 이 tensor들에 대한 gradient를 계산하려고 함을 PyTorch에 알립니다.\n",
    "\n",
    "Define a function z = x * y + x * x.\n",
    "\n",
    "함수 z = x * y + x * x를 정의합니다.\n",
    "\n",
    "Call z.backward(), which computes the gradient of z with respect to all of its dependencies (in this case x and y) using the chain rule.\n",
    "\n",
    "z.backward()를 호출하여 chain rule을 사용하여 z의 모든 종속성 (이 경우 x와 y)에 대한 z의 gradient를 계산합니다.\n",
    "\n",
    "Finally, print x.grad and y.grad. These are the gradients of z at x=2 and y=3. Since z=x*y + x*x, the derivative with respect to x is y + 2*x, and the derivative with respect to y is x. So at x=2 and y=3, the gradients should be y + 2*x=3+2*2=7 and x=2, respectively.\n",
    "\n",
    "마지막으로 x.grad와 y.grad를 출력합니다. 이것들은 x=2 및 y=3에서 z의 gradient입니다. z=x*y + x*x이므로, x에 대한 미분값은 y + 2*x이고, y에 대한 미분값은 x입니다. 따라서 x=2 및 y=3에서 gradient는 각각 y + 2*x=3+2*2=7와 x=2이어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36387abb",
   "metadata": {},
   "source": [
    "## Probability and Statistics\n",
    "\n",
    "Probability and statistics are crucial elements in deep learning. They help in dealing with uncertainties in data, adding probabilistic elements to model predictions, and often form the backbone of the optimization algorithms we use.\n",
    "\n",
    "확률과 통계는 딥러닝의 핵심 요소입니다. 이는 데이터의 불확실성을 처리하고, 모델의 예측에 확률적인 요소를 더하는 데 중요합니다. 또한, 최적화 알고리즘에서 확률적인 접근 방식을 사용하는 것이 일반적입니다.\n",
    "\n",
    "Probability: Probability quantifies the likelihood of occurrence of an uncertain event. The probability of an event lies between 0 and 1. There are three fundamental rules of probabilities:\n",
    "\n",
    "확률: 확률은 불확실한 사건의 발생 가능성을 측정하는 수치입니다. 사건의 확률은 0과 1 사이의 값을 가집니다. 확률의 세 가지 기본 규칙이 있습니다:\n",
    "\n",
    "1. The probability of an event is a non-negative real number: P(E) ≥ 0.\n",
    "        \n",
    "    어떤 사건의 확률은 0과 1 사이의 값을 가집니다.\n",
    "    \n",
    "    \n",
    "2. The probability of a certain event is 1: P(S) = 1.\n",
    "\n",
    "    확실하게 발생하는 사건(예: 주사위를 던져서 1 이상의 숫자가 나올 확률)의 확률은 1입니다.\n",
    "\n",
    "\n",
    "3. Any two mutually exclusive events A and B follow the rule: P(A ∪ B) = P(A) + P(B) for A ∩ B = ∅.\n",
    "\n",
    "    서로 배타적인 두 사건 A와 B의 합집합의 확률은 각 사건의 확률의 합과 같습니다(P(A ∪ B) = P(A) + P(B) for A ∩ B = ∅).\n",
    "    \n",
    "    \n",
    "    \n",
    "Statistics: Statistics deals with the theory and methods of using data for collection, analysis, interpretation, presentation, and modeling. It's crucial in deep learning to understand the data, extract features, and validate predictive models.\n",
    "\n",
    "통계: 통계는 데이터의 수집, 분석, 해석, 표현, 및 모델링에 사용되는 이론 및 방법을 다룹니다. 이는 딥러닝에서 데이터를 이해하고, 특성을 추출하며, 예측 모델을 검증하는 데 중요합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now let's consider some code related to probability and statistics using PyTorch. We'll first generate a probability distribution and visualize it.\n",
    "\n",
    "이제 PyTorch를 사용하여 확률 및 통계 관련 코드를 살펴보겠습니다. 우선 확률 분포를 생성하고, 이를 시각화해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0d194e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"235.7875pt\" height=\"169.678125pt\" viewBox=\"0 0 235.7875 169.678125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-06-28T16:00:52.194366</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 169.678125 \n",
       "L 235.7875 169.678125 \n",
       "L 235.7875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 33.2875 145.8 \n",
       "L 228.5875 145.8 \n",
       "L 228.5875 7.2 \n",
       "L 33.2875 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.164773 145.8 \n",
       "L 79.542763 145.8 \n",
       "L 79.542763 112.634171 \n",
       "L 42.164773 112.634171 \n",
       "z\n",
       "\" clip-path=\"url(#pe83605406d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 88.887261 145.8 \n",
       "L 126.265251 145.8 \n",
       "L 126.265251 78.473367 \n",
       "L 88.887261 78.473367 \n",
       "z\n",
       "\" clip-path=\"url(#pe83605406d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 135.609749 145.8 \n",
       "L 172.987739 145.8 \n",
       "L 172.987739 46.634171 \n",
       "L 135.609749 46.634171 \n",
       "z\n",
       "\" clip-path=\"url(#pe83605406d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 182.332237 145.8 \n",
       "L 219.710227 145.8 \n",
       "L 219.710227 13.8 \n",
       "L 182.332237 13.8 \n",
       "z\n",
       "\" clip-path=\"url(#pe83605406d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mca80022ff5\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mca80022ff5\" x=\"60.853768\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(57.672518 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mca80022ff5\" x=\"107.576256\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(104.395006 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mca80022ff5\" x=\"154.298744\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(151.117494 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mca80022ff5\" x=\"201.021232\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(197.839982 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path id=\"me802532be7\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#me802532be7\" x=\"33.2875\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(19.925 149.599219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me802532be7\" x=\"33.2875\" y=\"112.634171\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(7.2 116.43339) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me802532be7\" x=\"33.2875\" y=\"79.468342\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(7.2 83.26756) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me802532be7\" x=\"33.2875\" y=\"46.302513\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(7.2 50.101731) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me802532be7\" x=\"33.2875\" y=\"13.136683\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(7.2 16.935902) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 33.2875 145.8 \n",
       "L 33.2875 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 228.5875 145.8 \n",
       "L 228.5875 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 33.2875 145.8 \n",
       "L 228.5875 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 33.2875 7.2 \n",
       "L 228.5875 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pe83605406d\">\n",
       "   <rect x=\"33.2875\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import multinomial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 확률분포를 정의\n",
    "probabilities = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "# multinomial 분포에서 샘플링\n",
    "sample = multinomial.Multinomial(1, probabilities).sample((1000,))\n",
    "\n",
    "# 결과를 시각화\n",
    "counts = sample.sum(axis=0)\n",
    "plt.bar(range(len(probabilities)), counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b11c7",
   "metadata": {},
   "source": [
    "In the code above, we first define a probability distribution. Next, we generate samples from the distribution and visualize them. This code essentially picks values based on the given probability distribution and visualizes the number of times each value was picked.\n",
    "\n",
    "위의 코드에서는 먼저 확률 분포를 정의합니다. 다음으로, 이 분포에서 샘플을 생성하고, 이를 시각화합니다. 이 코드는 주어진 확률 분포에 따라 값을 선택하고, 각 값이 선택된 횟수를 시각화합니다.\n",
    "\n",
    "Next, let's look at an example of computing basic statistical measures (mean, variance, etc.).\n",
    "\n",
    "다음으로, 기초 통계량(평균, 분산 등)을 계산하는 예제를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f2f3022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of x: tensor(25.)\n",
      "Variance of x: tensor(166.6667)\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "x = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
    "\n",
    "# 평균 계산\n",
    "mean_x = torch.mean(x)\n",
    "print(\"Mean of x:\", mean_x)\n",
    "\n",
    "# 분산 계산\n",
    "var_x = torch.var(x)\n",
    "print(\"Variance of x:\", var_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71676d03",
   "metadata": {},
   "source": [
    "The code above first defines a tensor x and then calculates the mean and variance of the tensor. The mean indicates the central tendency of the values, while the variance shows how spread out the values are. These statistical measures can help you understand the overall characteristics of your data.\n",
    "\n",
    "이 코드는 먼저 텐서 x를 정의하고, 이 텐서의 평균과 분산을 계산합니다. 평균은 값의 중심 경향성을, 분산은 값의 퍼짐 정도를 나타냅니다. 이런 통계량은 데이터의 전반적인 특성을 파악하는 데 도움이 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41ee3",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Functions and Classes in a Module (모듈 내의 함수와 클래스)\n",
    "\n",
    "In order to know which functions and classes can be called in a module, we invoke the `dir` function. For instance, we can (**query all properties in the module for generating random numbers**):\n",
    "\n",
    "모듈에서 어떤 함수와 클래스를 호출할 수 있는지 알기 위해, 우리는 dir 함수를 사용합니다. 예를 들어, 우리는 (모듈에서 무작위 숫자를 생성하는 모든 속성을 쿼리할 수 있습니다):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6ba6528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7d8dc",
   "metadata": {},
   "source": [
    "Generally, we can ignore functions that start and end with `__` (special objects in Python) or functions that start with a single `_`(usually internal functions). Based on the remaining function or attribute names, we might hazard a guess that this module offers various methods for generating random numbers, including sampling from the uniform distribution (`uniform`), normal distribution (`normal`), and multinomial distribution (`multinomial`).\n",
    "\n",
    "일반적으로, 우리는 __로 시작하고 끝나는 함수(파이썬의 특수 객체)나 단일 _로 시작하는 함수(보통 내부 함수)를 무시할 수 있습니다. 남은 함수나 속성 이름을 기반으로, 이 모듈이 균일 분포(uniform), 정규 분포(normal), 다항 분포(multinomial)에서 샘플링을 포함한 다양한 무작위 숫자 생성 방법을 제공할 것으로 추측할 수 있습니다.\n",
    "\n",
    "\n",
    "### Specific Functions and Classes (특정 함수와 클래스)\n",
    "\n",
    "For more specific instructions on how to use a given function or class, we can invoke the  `help` function. As an example, let's [**explore the usage instructions for tensors' `ones` function**].\n",
    "\n",
    "주어진 함수나 클래스를 어떻게 사용하는지에 대한 더 구체적인 지침을 얻기 위해, 우리는 help 함수를 사용할 수 있습니다. 예시로, 텐서의 ones 함수에 대한 사용 지침을 (탐색해봅시다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8006ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones in module torch:\n",
      "\n",
      "ones(...)\n",
      "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
